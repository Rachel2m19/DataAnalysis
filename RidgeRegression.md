Ridge Regression 岭回归

特征变量之间存在很高的共线性（high collinearity）的情况下,建立线性回归或多项式回归将失败。
（共线性是自变量之间存在近似线性关系，会对回归分析带来很大的影响。）

我们进行回归分析需要了解每个自变量对因变量的单纯效应，高共线性就是说自变量间存在某种函数关系，
如果你的两个自变量间（X1和X2）存在函数关系，那么X1改变一个单位时，X2也会相应地改变，此时你无法做到固定其他条件，单独考查X1对因变量Y的作用，
你所观察到的X1的效应总是混杂了X2的作用，这就造成了分析误差，使得对自变量效应的分析不准确，所以做回归分析时需要排除高共线性的影响。

高共线性的存在可以通过几种不同的方式来确定：

• 尽管从理论上讲，该变量应该与Y高度相关，但回归系数并不显著。

• 添加或删除X特征变量时，回归系数会发生显着变化。

• X特征变量具有较高的成对相关性（pairwise correlations）（检查相关矩阵）。

我们可以首先看一下标准线性回归的优化函数，然后看看岭回归如何解决上述问题的思路：

其中X表示特征变量，w表示权重，y表示真实情况。岭回归是缓解模型中回归预测变量之间共线性的一种补救措施。由于共线性，多元回归模型中的一个特征变量可以由其他变量进行线性预测。

为了缓解这个问题，岭回归为变量增加了一个小的平方偏差因子（其实也就是正则项）：

这种平方偏差因子向模型中引入少量偏差，但大大减少了方差。

岭回归的几个要点：

• 这种回归的假设与最小平方回归相同，不同点在于最小平方回归的时候，我们假设数据的误差服从高斯分布使用的是极大似然估计（MLE），
在岭回归的时候，由于添加了偏差因子，即w的先验信息，使用的是极大后验估计（MAP）来得到最终参数的。

• 它缩小了系数的值，但没有达到零，这表明没有特征选择功能。
